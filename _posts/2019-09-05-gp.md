---
title: 'Gaussian Processes, not quite for dummies'
date: 2019-09-05
permalink: /posts/2019/09/GP/
tags:
  - Gaussian Processes
---

# Before diving in
I have procrastinated reading up about Gaussian process for many many moons. However, as always, I'd like to think that this is not just due to my extraordinary ability to slack. Whenever I look up "Gaussian Process" on Google, I find these well-written tutorials with vivid plots that explains everything up until non-linear regression in details, but shys away at the very first glimpse of any sort of information theory. The key takeaway is always, 
> A Gaussian process is a probability distribution over possible functions.

While memorising this sentence does help if some random stranger comes up to you on the street and ask for a definition of Gaussian Process -- which I'm sure happens all the time -- its function doesn't go much beyond that. In what range does the algorithm search for "possible functions"? What gives it the capacity to model things on a continuous, infinite space? 

Confused, I turned to the "the Book" in this area, _Gaussian Processes for Machine Learning_ by Carl Edward Rasmussen and Christopher K. I. Williams. I have friends working in more statistical areas (excuse the ambiguous term, don't want to get into a cat fight here) who swear by this book, but after spending half an hour just to read 2 pages about linear regression I went straight into an existential crisis. I'm sure it's a great book, but the math is quite out of my league.

So what more is there? Thankfully I found this [lecture by Dr. Richard Turner on YouTube](https://www.youtube.com/watch?v=92-98SYOdlY&t=4827s), which was a great introduction to GP, and some of its state-of-the-art approaches. After watching this video, reading the _Gaussian Processes for Machine Learning_ book became a lot easier. So I decided to compile some notes for the lecture, and hopefully if one day I do decide to put it on my website it will help some other people -- those who are eager to more than just scratch the surface of GP by reading some "machine learning for dummies" tutorial, but not quite has the claws to take on a textbook.

# Motivation: non-linear regression 
Of course, like almost everything in machine learning, we have to start from regression. Let's revisit the problem: somebody comes to you with some data points (red points in image below), and we would like to make some prediction of the value of y with a specific x.
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60500034-8bcb1f00-9cb1-11e9-9028-c2982528a5f2.png" alt="drawing" width="300"/></p>

In non-linear regression, we fit some nonlinear curves to observations. The higher degrees of polynomials you choose, the better it will fit the observations.  
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60500079-9be2fe80-9cb1-11e9-96ff-8f15bb902bd6.png" alt="drawing" width="300"/></p>

These sort of traditional non-linear regression, however, typically gives you **one** function that it considers to fit these observations the best. But what about the other ones that are also pretty good? What if we observed one more points, and one of those ones end up being a much better fit than the "best" solution?

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60500111-aac9b100-9cb1-11e9-8e18-181598397e29.png" alt="drawing" width="300"/></p>
To solve this problem, we turn to good old Gaussians.


# The world of Gaussians
## Recap
Here, it's useful to cover the basics of multivariate Gaussian distribution. If you're already familiar with this, skip to the next section `2D Gaussian Examples`.

### Definition
Multivariate Gaussian distribution is also known as joint normal distribution. It is the generalisation of univariate Gaussian in high dimensional space. Formally, the definition is:

> A random variable is said to be k-variate normally distributed if every linear combination of its k components have a univariate normal distribution.

Mathematically, $X = (X_1, ...X_k)^T$ has a multivariate Gaussian distribution if $Y=a_1X_1 + a_2X_2 ... + a_kX_k$ is normally distributed for any constant vector ${a} \in \mathcal{R}^k$.



_**Note**: if all k components are Gaussian random variables, then $X$ must be multivariate Gaussian (because the sum of Gaussian random variables is always Gaussian)._
_**Another note**: sum of random variables is different from sum of distribution -- the sum of two Gaussian distributions gives you a Gaussian mixture, which is not Gaussian except in special cases. Not sure why but I temporarily forgot about this and was extremely confused by everything I read about Gaussian for a very long time, so just putting this here more as a note to self._
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60735059-d36bd800-9f49-11e9-870a-9c6958afc964.png" alt="drawing" width="400"/></p>

### Independence conditions
If all components of ${X}$ are independent, ${X}$ is jointly normal. However, jointly normal components are only independent if they are not correlated.

## 2D Gaussian Examples
### Covariance matrix
Here is an example of a 2D gaussian distribution with mean 0, with the oval contours denoting points of constant probability.
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60500191-ccc33380-9cb1-11e9-8f80-5b695d896000.png" alt="drawing" width="300"/></p>
The covariance matrix, denoted as $\Sigma$, tells us (1) the **variance** of each individual random variable (on diagonal entries) and (2) the **covariance** between the random variables (off diagonal entries). The covariance matrix in above image indicates that $y_1$ and $y_2$ are positively correlated (with 0.7 covariance), therefore the somewhat "stretchy" shape of the countour. If we keep reducing the covariance while keeping the variance unchanged, the following transition can be observed:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60500266-f0867980-9cb1-11e9-8da6-f7f4fc5ab858.png" alt="drawing" width="220"/><img src="https://user-images.githubusercontent.com/18204038/60500289-faa87800-9cb1-11e9-84d0-50685c54a535.png" alt="drawing" width="220"/><img src="https://user-images.githubusercontent.com/18204038/60500307-03994980-9cb2-11e9-9efe-22cabecc0a27.png" alt="drawing" width="220"/></p>

Note that when $y_1$ is independent from $y_2$ (rightmost plot above), the contours are spherical.

### Conditioning
With multivariate Gaussian, another fun thing we can do is conditioning. In 2D, we can demonstrate this graphically:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60500452-478c4e80-9cb2-11e9-8df8-763938a0eca2.png" alt="drawing" width="200"/></p>

By drawing a verticle red line we fix the value of $y_1$ to compute the density of $y_2$ along the line -- thereby condition on $y_1$. Note that in here since $y_2 \in \mathcal{N}(\mu^*, \sigma_*)$ , by conditioning we get a gaussian back.

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60500477-4f4bf300-9cb2-11e9-9a90-7fc316f64468.png" alt="drawing" width="200"/></p>

We can also visualise how this conditioned gaussian changes as the correlation drop -- when correlation is 0, y1 tells you nothing about y2, so the mean drop to 0 and the covariance becomes high.

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60500538-6db1ee80-9cb2-11e9-9ee3-229ed8066366.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60500554-74d8fc80-9cb2-11e9-8856-18abc8d7c992.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60500573-7b677400-9cb2-11e9-9dba-e8761f9c9ca4.png" alt="drawing" width="200"/></p>



# High dimensional gaussian: a new interpretation
## 2D Gaussian
The oval contour graph of Gaussian, while providing information on the mean and covariance of our multivariate Gaussian distribution, does not really give us much intuition on how the random variables correlate with each other during the sampling process.

Therefore, consider this new interpretation that can be plotted following the below procedure:

> Take the oval contour graph of the 2D Gaussian -- along the bottom axis, we take an y1 component of the sample, and on the new graph, plot its value evaluated on y2 at index=1;
> <p align='center'><img src="https://user-images.githubusercontent.com/18204038/61443511-8d4b4700-a941-11e9-806b-202462ba27c1.png" alt="drawing" width="300"/></p>
> then similarly, take y2, plot its value on y1 at index=2;
> <p align='center'><img src="https://user-images.githubusercontent.com/18204038/60428229-4e528d00-9bf0-11e9-8813-9931dd159fb8.png" alt="drawing" width="300"/></p>

Under this new setting, we can now visualise the sampling operation. because y1 and y2 are correlated (0.9 correlation), the bar only "wiggles" ever so slightly as the two endpoints move up and down together. 

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60428356-91146500-9bf0-11e9-84e1-01af10afee47.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60428367-9671af80-9bf0-11e9-937c-7c73050e423d.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60428378-9bcefa00-9bf0-11e9-84f0-41408258c7d5.png" alt="drawing" width="200"/></p>

For conditioning, we can fix one of the endpoint on the index graph (in below plots, fix y1 to 1) and sample from y2.
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60428509-de90d200-9bf0-11e9-868a-4ecf745dbd39.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60428519-e3ee1c80-9bf0-11e9-8c9a-d9b5e6fa59e7.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60428532-ea7c9400-9bf0-11e9-9d2f-3123ae374283.png" alt="drawing" width="200"/></p>

## Higher dimensional Gaussian
### 5D Gaussian
Now we can consider a higher dimension Gaussian, starting from 5D. the covariance matrix is now 5x5. 

Take a second to have a good look at the covariance matrix, and notice: 

1. All variance (diagonal) are equal to 1;
2. The further away the indices of two points are, the less correlated they are. For instance, correlation between y1 and y2 is quite high, y1 and y3 lower, y1 and y4 the lowest)
 
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60428916-d5543500-9bf1-11e9-9176-4a9a029104d2.png" alt="drawing" width="300"/></p>

We can again condition on y1 and take samples for all the other points: we can notice that y2 is moving less compared to y3-y5 because it is more correlated to y1.
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60429176-662b1080-9bf2-11e9-909e-fe2910078bbc.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60429185-6fb47880-9bf2-11e9-975c-a8efa5716c62.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60429198-78a54a00-9bf2-11e9-8a86-f8a62c1bc296.png" alt="drawing" width="200"/></p>

### 20D Gaussian
To make things more intuitive, for 20D Gaussian we replace the numerical covariance matrix by a colour map, with warmer colors indicating higher correlation:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61445672-ba015d80-a945-11e9-95a8-e026a26f2856.png" alt="drawing" width="200"/>

Now look at what happens to the 20D gaussian conditioned on y1 and y2:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60429310-bb672200-9bf2-11e9-8030-6851f061185e.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60429330-c4f08a00-9bf2-11e9-8b27-7a72893e92fa.png" alt="drawing" width="200"/><img src="https://user-images.githubusercontent.com/18204038/60429352-cde15b80-9bf2-11e9-83c9-c638c0bb39a1.png" alt="drawing" width="200"/></p>

Hopefully some of you are now like: "Ah, this is looking exactly like the nonlinear regression problem we started with!" and yes, indeed, this is exactly like a nonlinear regression problem where y1 and y2 are given as observations. Using this index plot with 20D Gaussian, we can now generate **a family of curves** that fit these observations. What's better is, if we generate a number of them, we can compute the mean and variance of the fitting using these randomly generated curves. We visualise this in the plot below.

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61446482-1f098300-a947-11e9-8658-18c0b6e4d50d.png" alt="drawing" width="400"/></p>

We can see from the above image that because of how covariance matrix is structured (i.e. closer points have higher correlation), the points closer to the observations has very low uncertainty with non-zero mean, whereas the ones further from them have high uncertainty and zero mean. _(Note that in reality, we don't have to actually do Monte Carlo sampling to compute these mean and uncertainty, they are completely analytical.)_

Here we also offer a slightly more _exciting_ example where we condition on 4 points of the 20D Gaussian (and you wonder why everybody hates statisticians):

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61447393-c2a76300-a948-11e9-98da-5843098f7523.png" alt="drawing" width="200"/></p>


## Getting "real"

The problem with this approach for nonlinear regression seems obvious -- it feels like all x-axis have to be all integers because they are indices, while in reality, we want to model observations with real values. One immediately obvious solution for this is, we can keep increasing the dimensionality of the Gaussian and calculate many many points close to the observation, but that is a bit clumsy.

The solution lies in how the covariance matrix is generated. Conventionally, $\Sigma$ is calculated using the following 2-step process:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61448254-75c48c00-a94a-11e9-9150-79b03d54c694.png" alt="drawing" width="200"/></p>

The covariance matrices in all above examples are computed using the radial basis function (RBF) kernel $K(x_1, x_2)$ -- all by taking integer values for $x_1$, $x_2$. This RBF kernel ensures the **"smoothness"** of the covariance matrix, by generating a large function value for $x_1$ and $x_2$ that are closer to each other and small value for the ones that are further away. Note that if $x_1=x_2$, $K=\sigma^2$. We then take K and add $I\sigma_y^2$ for the final covariance matrix to factor in noise -- more on this later.

This means in principle, **we can calculate this covariance matrix for any real-valued $x_1$ and $x_2$ by simply plugging them in**. The real-valued xs effectively result in an infinite-dimensional Gaussian defined by the covariance matrix. 
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60430108-61fff280-9bf4-11e9-8ef4-a989734b859f.png" alt="drawing" width="400"/></p>

Now this is called a **Gaussian process** (mic drop).

# Gaussian Process
## Textbook definition
From the above derivation, you can view Gaussian process as a generalisation of multivariate Gaussian distribution to infinitely many variables. Here we also provide the textbook definition of GP, in case you had to testify under oath:


> A Gaussian process is a collection of random variables, any finite number of which have consistent Gaussian distributions.


Just like a Gaussian distribution is specified by it's mean and variance, a Gaussian process is completely defined by (1) a mean function $m(x)$ telling you the mean at any point of the input space and (2) a covariance function $K(x, x')$ that sets the covariance between points. Mean can be any value and the covariance matrix should be positive definite.

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61451937-80831f00-a952-11e9-9551-5e3d2646c2f3.png" alt="drawing" width="200"/></p>

## Parametric vs. non-parametric
Note that our Gaussian processes are non-paramatric, as opposed to nonlinear regression models which are parametric. And here's a secret: 

<h4 align='center'> non-parametric model == model with infinite number of parameters </h4>

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61577225-40e83e80-aadc-11e9-8b52-d001d25d7181.png" alt="drawing" width="200"/></p>

In a parametric model, we define the function explicitly with some parameters:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61698728-7df93e80-ad31-11e9-8b08-bc573c65fd0d.png" alt="drawing" width="120"/></p>

Where $\sigma_y$ is Gaussian noise describing how noisy the fit is to the actual observation  (graphically it'll represent how often the data lies directly on the fitted curve).
We can place a Gaussian process prior over the nonlinear function -- meaning, we assume that the parametric function above is drawn from the Gaussian process defined as follow:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61698798-9e28fd80-ad31-11e9-9146-12a9cdaac42c.png" alt="drawing" width="220"/></p>

This GP will now generate lots of smooth/wiggly functions, and if you think your parametric function falls into this family of functions that GP generates, this is now a sensible way to perform non-linear regression.

We can also add Gaussian noise $\sigma_y$ directly to the model, since the sum of Gaussian variables is also a Gaussian:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61699109-2d361580-ad32-11e9-8815-e0515a4b5aa0.png" alt="drawing" width="220"/></p>

In summary, GP is exactly the same as regression with parametric models, except you put a prior on the set of functions you'd like to consider for this dataset. The characteristic of this "set of functions" you consider is defined by the kernel of choice ($K(x, x')$). Note that the prior has mean 0.

## Hyperparameters
There are 2 hyperparameters here: 
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60430623-a2ac3b80-9bf5-11e9-952d-7181bbec066a.png" alt="drawing" width="400"/></p>

- **Vertical scale** $\sigma$: describes how much span the function has vertically;
- **Horizontal scale** $l$: describes how quickly the correlation between two points drops as the distance between them increases -- a high $l$ gives you a _smooth_ function, while lower $l$ results in a _wiggly_ function.

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61697612-82245c80-ad2f-11e9-8540-cef3b0715617.png" alt="drawing" width="400"/></p>

Luckily, because $p(y | \theta)$ is Gaussian, we can compute its likelihood in close form. That means we can just maximise the likelihood of $p(y|\theta)$ under these hyperparameters using a gradient optimiser:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61696910-1b527380-ad2e-11e9-8392-5a6bfe0bdfc0.png" alt="drawing" width="120"/></p>


## Details for implementation
_**Before we start:** here we are going to stay quite high level -- no code will be shown, but you can easily find many implementations of GP on GitHub (personally I like [this repo](https://github.com/dfm/gp/blob/master/worksheet.ipynb), it's a Jupyter Notebook walk through with step-by-step explanation). However, I would say this part is important to understanding how GP actually works, so try not to skip it._

### Computation
Hopefully at this point you are wondering: this smooth function with infinite-dimensional covariance matrix thing all sounds well and good, but how do we actually do computation with an infinite by infinite matrix?

**Marginalisation baby!** Imagine you have a  multivariate Gaussian over two vector variables $y_1$ and $y_2$, where:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61701083-7b98e380-ad35-11e9-8012-5b86e1299cf5.png" alt="drawing" width="250"/></p>

Here, we partition the mean into the mean of $y_1$, $a$ and the mean of $y_2$, $b$; similarly, for covariance matrix, we have $A$ as the covariance of $y_1$, $B$ that of  $y_1$ and $y_2$, $B^T$ that of  $y_2$ and $y_1$ and $C$ of $y_2$.
So now, we can easily compute the probability of $y_1$ using the marginalisation property:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61701492-2b6e5100-ad36-11e9-8ad8-7c2478b29d4d.png" alt="drawing" width="500"/></p>

Which means, we can just put all the points that we are interested in in one partition ($y_1$) and compute mean and covariance for that partition only, and shove the rest of the infinte stuff into $y_2$ and not worry about computing them.
This nice little property allows us to think about finite dimensional projection of the underlying infinite object on our computer. We can forget about the infinite stuff happening under the hood.

### Predictions
Taking the above $y_1$, $y_2$ example, but this time imagine all the observations are in partition $y_2$ and all the points we want to make predictions about are in $y_1$ (again, the infinite points are still in the background, let's imagine we've shoved them into some $y_3$ that is omitted here).
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61702038-09290300-ad37-11e9-911a-346d4e5a9727.png" alt="drawing" width="250"/></p>

To make predictions about $y_1$ given observations of $y_2$, we can then use bayes rules to calculate $p(y_1|y_2)$:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61702480-c156ab80-ad37-11e9-94e7-8c7989fefd0a.png" alt="drawing" width="150"/></p>

Because $p(y_1)$, $p(y_2)$ and $p(y_1,y_2)$ are all Gaussians, $p(y_1|y_2)$ is also Gaussian. We can therefore compute $p(y_1 | y_2)$ analytically:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61703635-ea783b80-ad39-11e9-9606-54b5995df95c.png" alt="drawing" width="300"/></p>

> Note: here we catch a glimpse of the bottleneck of GP: we can see that this analytical solution involves computing the inverse of the covariance matrix of our observation $C^{-1}$, which, given $n$ observations, is an $O(n^3)$ operation. This is why we use Cholesky decomposition -- more on this later.

To gain some more intuition on the method, we can write out the predictive mean and predictive covariance as such:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61703783-3e832000-ad3a-11e9-9139-3a478550e1be.png" alt="drawing" width="500"/></p>

So the mean of $p(y_1|y_2)$ is linearly related to $y_2$, and the predictive covariance is the prior uncertainty subtracted by the reduction in uncertainty after seeing the observations. Therefore, the more data we see, the more certain we are.

### Higher dimensional data
You can also do this for higher-dimensional data (bear in mind the computational costs). Here we extend the covariance function to incorporate RBF kernels in 2D data:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60501013-32fc8600-9cb3-11e9-84db-39fd11a947f5.png" alt="drawing" width="500"/></p>

### Covariance matrix selection
As one last detail, let's talk about different forms of covariance matrix that is commonly used for GP. Again, all positive definite matrices should qualify, but here are some frequently seen ones:
#### Laplacian function
This function is continuous but non-differentiable. It looks like this:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61705455-adae4380-ad3d-11e9-848b-0f9c49f85b99.png" alt="drawing" width="350"/></p>
If you average over all samples, you get straight lines joining your datapoints, which are called Brownian bridges.
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60494736-430e6880-9ca7-11e9-812b-b7944305d16f.png" alt="drawing" width="150"/><img src="https://user-images.githubusercontent.com/18204038/60494726-3c7ff100-9ca7-11e9-864d-e41619f2bd5c.png" alt="drawing" width="150"/></p>

#### Rational quadratic
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61705556-e0583c00-ad3d-11e9-90d7-177c1cdb5f66.png" alt="drawing" width="350"/></p>

Average over all samples looks like this:
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/60499570-a18c1480-9cb0-11e9-99a4-87e02f195c9e.png" alt="drawing" width=150"/><img src="https://user-images.githubusercontent.com/18204038/60499556-9b963380-9cb0-11e9-9f1c-825484a88a74.png" alt="drawing" width="150"/></p>

#### Periodic functions
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61706968-55794080-ad41-11e9-81ee-21043a296a08.png" alt="drawing" width="350"/></p>

Average over all samples looks like this:
<p align='center'><img src=https://user-images.githubusercontent.com/18204038/61706926-3ed2e980-ad41-11e9-857b-4f38b504cf95.png" alt="drawing" width="150"/><img src="https://user-images.githubusercontent.com/18204038/61706395-e9e2a380-ad3f-11e9-86dc-713f14db3bae.png" alt="drawing" width="150"/></p>

#### Summary
There are books that you can look up for appropriate kernels for covariance functions for your particular problem, and rules you can follow to produce more complicated covariance (like the product of two covariance functions is a valid covariance). They can give you very different results.
<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61706573-565da280-ad40-11e9-848a-10f34fa1113b.png" alt="drawing" width="500"/></p>

It is tricky to find the appropriate covariance, but there are also methods in place for model selection. One of those methods is Bayesian model comparison, defined as follow:

<p align='center'><img src="https://user-images.githubusercontent.com/18204038/61706770-d5eb7180-ad40-11e9-9235-b802afec85c4.png" alt="drawing" width="500"/></p>

However, it does involve a very difficult integral (or sum in discrete case, as showcased above) over the hyperparameters of your GP, which makes it impratical. it's also very sensitve to the prior you put on your hyperparameters. 

Deep Gaussian processes avoids this and picks the covariance function for you.

